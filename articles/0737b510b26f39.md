---
title: "Hands on: D-Robotics RDK X3とROS2のきほん"
emoji: "🤖"
type: "tech"
topics:
  - "python"
  - "ros"
  - "ros2"
  - "電子工作"
published: true
published_at: "2025-12-21 12:32"
---

みんな、こんにちは！AIロボットとっても流行っているよね！みんなは、そんなAIロボットを自分で簡単に作ってみたいと思ったことはある？ボクは興味はあったけど、AIロボットなんてボクが作れるの？って感じだったよ。でも、そんなみんなの願いを実現できるシングルボードコンピューター（SBC）を中国のD-Roboticsが作っていて、日本でもスイッチサイエンスで売られているよ。それがRDK X3やX5といったAIの推論チップ搭載のSBCだよ。

https://ssci.to/9836

今回はRDK X3がどういうものなのか？誰でもお手軽に使えるの？とかみんなが知りたそうなところを実際に使ってまとめてみた。というのと、ロボットの世界では当たり前に使われているROSというロボット用のフレームワークについて、その基本もまとめてみたよ。（この記事ではROSはROS2のことを指しているよ）どうしてROSについてもまとめたかっていうと、X3はラズパイ型のSBCだからROSを使わなくてもロボットを作ることはできるんだけど、X3が持つAIの推論能力をお手軽に活用するにはD-Roboticsが提供しているTogetheROS.Bot（以下tros.b）というROSの拡張を使うことになるから、必然的にROSの知識が求められることになるよ。

ボクはX3のことも、ROSのこともまだまだ勉強中だから、足りないこともあるかもしれないけど一緒に勉強していこうね！

X3はこういうこと👇が簡単にできるよ
https://x.com/nananauno/status/1999126523257958684?s=20

## やること
- RDK X3を使える状態にする
- RDK X3とWebカメラを使ってジェスチャー認識のデモを動かしてみる
- ROS2のノード、トピックについて理解する
- ジェスチャー認識の結果をトピックで取得する

ボクは最終的にはカメラで身振り手振りを認識できるカワイイAIロボットを作りたいんだけど、今回はX3の基本的な使用方法とジェスチャー認識のデモ、ROSのトピックを使用してジェスチャーの値を取得できるところまでやっていくよ。

ノード？トピック？これはROSで使われている仕組みだけど、後で説明するから今は分からなくても大丈夫！

## こんな人にオススメ
- ROSってなに？
- エッジAIやAIロボットに興味がある
- RDK X3/X5に興味がある

X3はラズパイ型（形はホントにラズパイ）のSBCでUbuntuというLinuxが動いているから、X3を使用するには少しだけLinuxの知識が求められるよ。あと、X3を使ってロボットを作るにはプログラミングも必要になって、この記事ではpythonを使って説明しているよ。

## 準備
RDK X3の概要とX3を使えるようにするまでの流れをまとめているよ。

### RDK X3って？
RDK X3は中国のD-Robotics（地瓜机器人）が作ったAI推論チップ搭載のラズパイ型SBCだよ。D-Roboticsは中国語だとサツマイモロボティクスってカワイイ名前で、なんでサツマイモ？というのはサツマイモはどんな過酷な環境でも根を張って育って人類を養う食べ物で、D-Roboticsもどんな環境でも根を張ってロボット産業を支えるプラットフォームになりたいという思いからこの名前になっているみたいだよ。

D-Roboticsは中国のHorizon Robotics(ホライゾン)という自動車向けのAIチップを作っているの会社からスピンアウトしたAIロボット向けシングルボードコンピューター（SBC）を作っている会社だよ。ホライゾンは色々な自動車メーカー向けに自動運転用のAIチップを供給していて、D-Roboticsはホライゾンが開発したAIチップを搭載したラズパイ＋AIみたいなSBCを作っているよ。中国ではロボット界のデファクトスタンダードになりつつある？みたい。D-Roboticsが作っているSBCはRDK(Robot Development Kit) X3やX5という名前で、日本ではスイッチサイエンスが販売しているよ。X3は2024年に日本で発売されて、推論能力がX3の2倍になったX5が2025年に発売されているよ。

ここではスペックなどの詳細は説明しないから、その辺りは公式サイトなどを見てね。

https://developer.d-robotics.cc/rdk_doc/en/Quick_start/hardware_introduction/rdk_x3

### 外観
見た目は完全にRaspberry Piだね。X3のボードサイズは56mm×85mmでラズパイと完全一致。40ピンGPIOは配置もピン割り当ても同じみたい。端子の位置はちょっと違うからケースの流用はできないものもあるかもね。

![](https://storage.googleapis.com/zenn-user-upload/5ac273c817ca-20251220.jpeg =500x)
*ラズパイ？*

### 環境
X3を動かすために必要な環境は以下の通りだよ。用意するものはちょっと多めだけど頑張って！
- RDK X3本体
- RDK X3用ヒートシンク
- microSDカード（32GB以上推奨）
- モニター（HDMI）
- HDMIケーブル
- USBキーボード＆マウス（BluetoothでもOK）
- USB Webカメラ（ジェスチャー認識用）
- PC（OSイメージ書き込み用、Windows/MacどちらでもOK）
- 電源（5V/3A以上のUSB-C）

マウスとキーボードはBluetoothでも接続することができるよ。

### 初期セットアップ
X3用のOSイメージを取得して、microSDカードに書き込み、Ubuntuを起動できるところまでを説明しているよ。

初期セットアップ方法の公式ドキュメントはこちら👇
https://developer.d-robotics.cc/rdk_doc/en/Quick_start/install_os/rdk_x3

セットアップの流れとしては以下の通りだよ：
1. 公式サイトからOSイメージをダウンロード
1. Raspberry Pi ImagerでmicroSDカードに書き込む
1. microSDを挿して電源ON！

各手順の詳細は以下の通りだよ。

**1. 公式サイトからOSイメージをダウンロード**
https://archive.d-robotics.cc/downloads/os_images/
ここから最新のOSイメージをダウンロードしよう。ボクはrdk_os_3.0.3-2025-09-08を使ったよ。ダウンロードに結構時間がかかるから覚悟してね。

**2. Raspberry Pi ImagerでmicroSDカードに書き込む**
OSイメージの書き込み方はからあげ先生の記事を参考にしたよ。
https://zenn.dev/karaage0703/articles/cfd05ed36e2698

Raspberry Pi Imagerを入手してmicroSDカードにOSイメージを書き込もう。Raspberry Pi Imager v1.9.xだとOSイメージでカスタムイメージを選択して手順１でダウンロードしたイメージを選択して、ストレージでmicroSDカードを選択するだけだよ。

**3. microSDを挿して電源ON！**
マウス、キーボード、モニターをX3に接続した状態で電源を供給すると、Ubuntuが立ち上がってくるよ。Ubuntuのユーザー名・パスワードは両方 `sunrise`に設定されているよ。Ubuntuが立ち上がったあとはWi-Fi設定をしておいてね。

以上でX3が起動して使える状態になるよ。

### TogetheROS.Bot
TogetheROS.Bot、略してtros.bはX3の機能をフル活用するためにD-Roboticsが提供しているROS2の拡張だよ。X3用のOSイメージにプリインされたtros.bはROS2のHumbleというバージョンになっているよ。tros.bにはX3の推論チップ（D-RoboticsはBPU(Brain Processing Unit)と呼んでいる）をフル活用できるサンプル（物体検出・ジェスチャー認識・音声認識など）も入っているから、すぐにAI機能を試すことができるよ。
https://developer.d-robotics.cc/rdk_doc/en/Robot_development/tros

tros.bのインストール方法は公式サイトに記載されているけど、ボクが使用したOSイメージには最初から入っていたよ。ボクは以下のコマンドで最新の状態にはしておいたよ。

```bash
sudo apt update
sudo apt upgrade
```

もし入ってなかったら、以下のコマンドでインストールしてね。
```bash
sudo apt update
sudo apt install tros-humble
```

ここまでできたら、X3を使う準備はOKだよ。あとは実際に動かしてみよう。これ以降はいくつかコマンドを実行したりするけど、コマンドを実行するためにターミナルを立ち上げた後は以下のコマンドを実行する必要があることを覚えておいてね。複数のターミナルを立ち上げるときは、それぞれのターミナルで実行してね。

```bash
source /opt/tros/humble/setup.bash
```

このコマンドはROSを動かすための初期設定みたいなものだよ。

## ジェスチャー認識のデモ
tros.bには最初から推論チップを使うためのサンプルが用意されているよ。今回、ボクのカワイイAIロボットはカメラ画像からジェスチャーを認識して反応するようなものを作りたいから、ジェスチャー認識のデモを動かしてみるよ。

まず、ターミナルを立ち上げたら忘れずに以下のコマンドを実行してね。

```bash
source /opt/tros/humble/setup.bash
```

あとは、以下のコマンドを順番に実行するだけ。

```bash
# サンプルの実行に必要な設定ファイルをコピーするよ
cp -r /opt/tros/${TROS_DISTRO}/lib/mono2d_body_detection/config/ .
cp -r /opt/tros/${TROS_DISTRO}/lib/hand_lmk_detection/config/ .
cp -r /opt/tros/${TROS_DISTRO}/lib/hand_gesture_detection/config/ .

# USBカメラを使用するために以下の設定をするよ
export CAM_TYPE=usb

# あとは実行するだけ
ros2 launch hand_gesture_detection hand_gesture_detection.launch.py
```
デモを実行するとログがたくさん出力されてくると思うけど、これだけだとホントにジェスチャーが認識されているかよく分からないよね。そこで、PCのブラウザで
```http://X3のIPアドレス:8000/```
を開いてみてね。このURLを開くとX3で動作中のジェスチャー認識の状態を確認することができるよ。カメラの前でピースしたりサムアップしたりして変化を確認してみてね。

tros.bには他にも色んなAIのデモが入っているから試してみてね。デモだけでも十分にAIロボットに使えるんじゃないかな？

## ROS2を理解する
ここで、少しだけROSの説明をするよ。今はROSといえばROS2のことだから、ここでもROS=ROS2の話になっているよ。最初にも説明した通り、X3のAI性能を活用するにはD-Roboticsが提供するtros.bというROSの拡張を使うのが最もお手軽だよ。

### ROSって？
ロボットに必要な要素（部品）ってなんだろう？手足を動かすサーボモーター、周囲の状況を撮影するカメラ、色んな情報を得るためのセンサー、お顔を表示するディスプレイ・・などたくさんの要素で構成されているよね。ロボットのソフトを作ろうと思った時に、これらの要素を１つのプログラムでまとめて開発しようとするととっても大変だよね。センサーの情報を取得して、センサーの値に応じてサーボを動かしたり、画面を書き換えたりって色んな処理をまとめることは考えただけでも頭が真っ白になっちゃうよね。

ROS(Robot Operating System)はそんなお悩みを解決するためのフレームワーク。名前にOSって付いてるけど、実際にはOSではなくてロボット用の便利なフレームワークだね。ボクが（たぶんほとんどの人が）思うROSの最大の特徴は**ノード**という概念だよ。

### ノード

最初にロボットを構成する要素をいくつか挙げたけど、ノードはこのロボットの要素１つ１つに対応するプログラムの単位と思っておけば大丈夫だよ。たくさんの要素を１つにまとめるんじゃなくて、要素ごとに１つのノードというプログラムに分けてそれを組み合わせるという考え方だよ。ノード単位で開発することで、１つのノードはその要素のための処理だけを作ればいいから、複雑なシステムでもすごくシンプルに考えることができるよね。

![](https://storage.googleapis.com/zenn-user-upload/4f3e2e713642-20251220.png)
*ノードは要素ごとの処理だけに集中できるよ*

上の図にはロボットによくある要素、カメラとサーボモーターの制御、そして全体の動きを司るコントローラーを並べているけど、カメラの画像を処理するノード、サーボモーターの制御をするためのノード、コントローラーのノードにそれぞれ分かれているよ。例えばサーボモーターの制御ノードは指定された角度に移動する処理だけを実装すれば良いよ。

ノードで要素ごとにプログラムを作れることは分かったけど、じゃあロボットのシステムとしてどうやってノード同士が連携するのかというと、**トピック**という仕組みを使ってデータをやり取りするよ。

### トピック
トピックはノード間で簡単にデータをやり取りするための仕組みだよ。データを送る側はトピックに対してPublish、データを受け取る側はトピックをSubscribeするよ。PublishとSubscribeは常にデータのやり取りが行われる必要もなくて、必要な時にだけPublishでデータを送ったり、Subscribeでデータを受け取ったりすることができるよ。トピックはone-to-one, many-to-one, one-to-many, many-to-manyどんな形でもOKだよ。

ROSのトピックで面白いのが、このデータのやり取りは単一のシステム上（例えばRDK X3）だけじゃなくて、同じネットワークで接続されていれば、別のシステムのノードとも通信できるところだよ。ロボットの制御はRDK X3上で、重たい画像処理はハイスペックなPC上でのような使い方が簡単にできるようになっているよ。

![](https://storage.googleapis.com/zenn-user-upload/f1ca04708fee-20251220.png)
*トピックを使えばノード間の連携が簡単にできる*

さっきの例だと、カメラで処理した結果のやり取りと、サーボを制御するための角度の指定が必要そうだね。上の図だと、/camera_detectionというトピックと/target_angleという２つのトピックを用意してデータのやり取りをしているよ。カメラのノードはカメラ画像の処理結果を常時/camera_detectionにPublishしていれば良くて、コントローラーのノードは必要な時だけ/camera_detectionをSubscribeしてカメラの処理結果を受け取ることができるよ。コントローラーはカメラの処理結果に応じて/target_angleにサーボの角度をPublishすることで、サーボのノードは指定された角度に動くことができるよ。

## トピックを使ってジェスチャーの結果を取得する
ROSの基本が理解できたところで、少し前にやったジェスチャー認識の結果をトピックから取得してみるよ。ここではコマンドライン上で確認する方法とpythonでトピックをSubscribeする方法の２つを説明しているよ。

### コマンドラインで取得する
トピックからデータを受け取るには、トピックの名前を知っている必要があるよ。現在有効なトピックの一覧は以下のコマンドで取得できるよ。

```bash
ros2 topic list
```

コマンドを実行すると、たぶん👇のような結果が帰ってくるよ。ここに表示されたトピックは今有効になって使えるってことだね。この中の```/hobot_hand_gesture_detection```がジェスチャー認識の結果のトピックになっているよ。

```bash
/camera_info
/hbmem_img
/hobot_hand_gesture_detection
/hobot_hand_lmk_detection
/hobot_mono2d_body_detection
/image
/parameter_events
/rosout
```

```bash
ros2 topic echo /hobot_hand_gesture_detection
```

このコマンドを実行すると```/hobot_hand_gesture_detection```に対してPublishされているメッセージを表示することができるよ。情報が多くて分かりづらいと思うから、Ctrl+Cで実行を止めてから、以下のコマンドでgesture関連の情報だけ絞り込んでみよう。grepコマンドにパイプで渡して、-Aオプションで一致部分+2行を出力するようにしているよ。

```bash
ros2 topic echo /hobot_hand_gesture_detection | grep gesture -A 2
```

うまくいけば、以下のように認識されたジェスチャーの値が見れるはずだよ。valueがジェスチャーの種類だね。

```bash
  - type: gesture
    value: 2.0
    confidence: 0.0
```

2.0はThumbUpで、この値が示すものは公式ドキュメントのGesture Recognitionのページに記載されているよ。
https://developer.d-robotics.cc/rdk_doc/en/Robot_development/boxs/body/hand_gesture_detection

トピックの値はこんな感じで簡単に取得することができることが分かってもらえたかな？

### pythonでSubscribeする
最後は、さっきコマンドラインで取得したトピックの内容をpythonで作ったプログラムから取得する方法について説明するよ。自分でロボットを作るなら、pythonでトピックの内容を取得して内容に応じた処理を行うことになるから、次のステップとしてpythonでトピックの内容を取得する方法は必ず知っておく必要があるよね。

以下はpythonで```/hobot_mono2d_body_detection```トピックをサブスクして内容を取得するための基本的なコードだよ。ROSではパッケージという概念もあって、ちゃんとしたロボットのコードを作成するときはパッケージで作成する必要はあると思うけど、試しにトピックの内容を取得するだけならこのコードを単体で作成して実行するだけでオッケーだよ。

```python
import rclpy
from rclpy.node import Node
from ai_msgs.msg import PerceptionTargets
# ↑トピックはメッセージの型があるんだけど、ジェスチャーのデモはこの型が使用されているよ。
# メッセージの型は以下のコマンドで確認できるよ
# ros2 topic info /hobot_mono2d_body_detection

class GestureListener(Node):
    def __init__(self):
        super().__init__('gesture_listener')
        self.subscription = self.create_subscription(PerceptionTargets, '/hobot_mono2d_body_detection', self.callback, 1)

    def callback(self, msg):
        self.get_logger().info(f'検出されたターゲット数: {len(msg.targets)}')

        for target in msg.targets:
            self.get_logger().info(f'type: {target.type}')
            self.get_logger().info(f'要素数: {len(target.attributes)}')
            # ここでホントは要素が取得できるはずなんだけど、今は取れていないよ。

def main():
    rclpy.init()
    node = GestureListener()
    rclpy.spin(node)

if __name__ == '__main__':
    main()
```

ジェスチャーの認識デモを動かした状態でこのコードを実行してみてね。以下のようなログがたくさん出力されるはずだよ。これで自分のpythonコードからトピックをサブスクしてジェスチャーの認識デモの結果を取得できるようになったね！

```bash
[INFO] [1766282452.260065857] [gesture_listener]: 検出されたターゲット数: 3
[INFO] [1766282452.262513724] [gesture_listener]: type: person
```

ただ、ボクはこのコードでまだ課題が残っていて、本来は```target.attributes```の中にジェスチャーのIDが入っているはずなんだけど、このコードではうまく取得できなかったよ。コマンドラインの```ros2 topic echo```では確かにattributesに値が入っているんだけど、pythonから取得できていない原因はまだ調査中だよ。原因が分かったら更新するね。

## まとめ
D-RoboticsのAIロボット向けSBCであるRDK X3を使える状態にするまでとジェスチャーの認識デモを動かしてトピックの内容をコマンドラインとpythonで取得する方法についてまとめたよ。pythonでのジェスチャー認識結果にはまだ課題が残っているんだけど、 X3の基本的な使い方は理解できたかな？どう？X3を使うとAIロボットが簡単に作れそうな気がしてくるんじゃないかな？

ボクはラズパイみたいなSBCが好きじゃなくて、今までは使ってこなかったんだけど、今回X3を使ってみてSBCの良いところも発見できたし、性能が求められるAIロボット向けのソリューションとしてのラズパイ型＋推論チップのようなボードはとってもしっくりきたよ。あと、ROSも全然知らない状態でX3を触り始めたんだけど、ROSの基本が理解できてとっても良かったね。ROSのノードとトピックという考え方、とっても好き。どんな複雑なロボットでも要素ごとにノードという単位で開発できることで、ロボット開発の難易度はグッと下がりそうだね。

ボクの次のステップとしては、サーボーモーター用のノードや全体のコントローラーノードを作って、ジェスチャーの結果と連携することが次のステップかな。みんなもX3とROSを使って自分だけのAIロボット作りに挑戦してみてね。

じゃあ、またね！

## 参考資料
https://kanpapa.com/2024/09/rdk-x3-roomba-controlled-ros2.html
https://note.com/npaka/n/nb44bba82faab
